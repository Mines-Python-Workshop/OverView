# Python Workshop Day 1 
Welcome to the Chemistry Department Programming Workshop Day 1!  
   
These tutorials will focus on...
  
Day 1 consists of three sub-modules:  
* 1.1 External Data Files
* 1.2 TBD
* 1.3 Plotting With Matplotlib

## 1.1 External Data Files
---

**Contents**  
  
In this module, you will:
 * 1.1.0 Import packages
 * 1.1.1 Import external data files
 * 1.1.2 Query data
 * 1.1.3 Debug common data importing issues
 * 1.1.4 Import multiple files simultaneously
 
## 1.1.0 Importing packages
'**NumPy** is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked 
arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier 
transforms, basic linear algebra, basic statistical operations, random simulation and much more' (from the NumPy documentation [site](https://numpy.org/doc/stable/)).

We will use the **NumPy** package as one of the options for importing external data. To import the package:

```
import numpy
```

I often use an abbreviated name when importing packages, which allows us to call **numpy** using an abbreviated keyword.

```
import numpy as np
```

---
'**Pandas** is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.'
(from the Pandas documentation [site](https://pandas.pydata.org/))

```
import pandas as pd
```

**Pandas** contains two basic data structures: 
1) `Series`: a one-dimensional labeled array holding data of any type (such as integers, strings, Python objects, etc.)
2) `DataFrame`: a two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns
 
## 1.1.1 Importing external data files
In this section, we will cover some common methods for importing external data files within native Python and with two different Python packages: **numpy** and **pandas**.

### File handling in native Python
---
Python has a built-in function for opening files in different modes, `open(fname, mode)`, where *fname* corresponds to the file name. There are five file modes to select from:
- `'r'`: opens a file to read only
- `'w'`: opens a file for writing or creates a new file to write to (depending on if the file exists already)
- `'a'`: opens a file for appending only
- `'x'`: creates a new file
- `'+'`: opens a file for updating

We will focus on the first two, reading and writing files. Let's see how these modes work using the text file *mines.txt* (found under *Workshop_Day1/Data/*).
To open and read this file, we first want to use the `open` function and get an output of the text file as an object, assigned to the variable *f*.

```
f = open('./Data/mines.txt', 'r')
```

Next, we will want to actually parse the file using the `read` method. Putting this method within a `print` statement will display the text from the file for us to view.

```
print(f.read())
```

When using the `open` function, the `close` method must always be called when done with the opened file.

```
f.close()
```

Alternatively, we can use the `with` context manager instead of a `close` statement.

```
with open('./Data/mines.txt', 'r') as f:
	print(f.read())
```

We can also read a single line of a file with `readline`. This code snippet prints just the first line of the text file:

```
with open('./Data/mines.txt', 'r') as f:
	print(f.readline())
```

Similarly, we can read each line of a text file and store each line as an element of a list.

```
with open('./Data/mines.txt', 'r') as f:
	lines = f.readlines()
	
print(lines)
```

### Importing files with numpy
---
To load a standard text (*.txt*) file (or *.csv*, *.xy*, etc.) with **numpy**, we can use `loadtxt` to import a file and return the data as an array. 
The only required parameter is the name of the file being imported (`fname`), but many other optional parameters are available depending on the formatting of the file: 
Below is a complete list of parameters for `loadtxt` and their default values:

```
loadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding=None, max_rows=None, quotechar=None, like=None)
```

- `fname` (str): name of file being imported, including its location and extension
- `dtype` (data-type): data-type of the resulting array, default is 'float'
- `comments` (str): characters (or a list of characters) used to indicate a comment, default is '#'
- `delimiter` (str): character used to separate values, default is whitespace
- `converters` (dict): functions for customizing value parsing 
- `skiprows` (int): number of lines at the beginning of the file to skip (includes comments), default is 0
- `usecols` (int): which columns to read, with 0 being the first (ex. usecols=(1,4,5) extracts the 2nd, 5th, and 6th columns), default is all columns are read
- `unpack` (bool): if True, arguments may be unpacked using `x, y, z = loadtxt(...)`, structured data-type will return an array for each field, default is False
- `ndmin` (int): resulting array will have at least *ndmin* dimensions, must be 0 (default), 1, or 2
- `encoding` (str): encoding to decode the input file, default is 'bytes'
- `max_rows` (int): reads *max_rows* number of rows of data after *skiprows* number of lines, default is all rows are read
- `quotechar` (unicode character): character denoting the start and end of a quoted item, default is None (quoting support disabled)
- `like` (array_like): allows creation of non-numpy arrays

(*Reminder of data-types: str is string, dict is dictionary, int is integer, bool is boolean*)

For the purposes of this tutorial, we will only focus on commonly used optional parameters, namely: dtype, comments, delimiter, skiprows, and unpack.
Further details on the parameters not covered here can be found in the [loadtxt documentation](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html).

As an example, I've used a random number generator to generate 50 numbers from a Gaussian distribution (mean=10, σ=1.0). 
Let's import this text file (found under *Workshop_Day1/Data/rand_gaussian.txt*) using `loadtxt`.

```
data = np.loadtxt('./Data/rand_gaussian.txt', skiprows=1)
```

In the above statement, we've specified the file we want to import first, which is always required (`fname`). 
The the default values of `dtype` and `delimiter` are appropriate and do not need to be specified since the data we're importing contains float values and is only a single column (no delimiter).
Additionally, we've used the optional parameter `skiprows` to indicate that the first row of the text file is not data we want to include (since it's a header).
Since the default value of `comments` is *'#'*, we don't need to specify this as a parameters, since the header comment line in *rand_gaussian.txt* is also *'#'*.

As another example, let's import a text file with some X-ray diffraction data of silicon (found under *Workshop_Day1/Data/Si_Expt.txt*). 
If you open this file in a text editor, you will see there is some header information followed by two columns of data, 2θ and intensity, instead of just a single column of data.

The first option for importing this data is to return a two-dimensional array, where the first column corresponds to the first element and the second column the second array element.
In this case, each element of the resulting array is also an array.

```
data_2d = np.loadtxt('./Data/Si_Expt.txt', skiprows=2)
```

This second option is to set the `unpack` parameter to be True, allowing for each column to be returned as its own separate data array. 
Since there are two data columns, we need to use two variable names in the `loadtxt` statement, one for each returned array.

```
two_theta, intensity = np.loadtxt('./Data/Si_Expt.txt', skiprows=2, unpack=True)
```

Similar to the first example, `dtype`, `comments`, and `delimiter` can be left as their default values, so we don't need to include them as parameters, 
and `skiprows` is set to 2 to exclude the first two rows, which contain the header information.

As a final example, let's import a CSV file with text as data instead of numbers (found under *Workshop_Day1/Data/fruits.csv*). 
This file has a list of fruits with peels in the first column, and a list of fruits without peels in the second column.

```
peel, no_peel = np.loadtxt('./Data/fruits.csv', dtype='str', comments='%', delimiter=',', skiprows=1, unpack=True)
```

In this statement, we've had to specify more parameters as they differ from the default values. 
First, we need to specify that our data consists of strings, not floats, by setting `dtype` to *'str'*. This will ensure the returned arrays have string elements.
The header comment in this file is denoted with *'%'* instead of the conventional *'#'*, so we need to set `comments` to *'%'*. Additionally, as we have one commment line, `skiprows=1`.
Since we're loading a CSV file, the data in each row are separated by commas, meaning we need to specify `delimiter` accordingly.
Lastly, since we have multiple columns of data to import as separate arrays, we will set `unpack` to True.

Another useful numpy function for importing data is `genfromtxt`, which handles data with missing values by either masking out missing values or filling them in as specified. 
The details will not be covered in this tutorial, but can be found [here](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html) if interested in learning more.

### Importing files with pandas
---
**Pandas** contains a very powerful object class called a `DataFrame`, which is a 2-D data structure and is incredibly useful for working with tabulated data. 
Here, I will highlight some of the most common input tools, but I encourage the more advanced user to look at the 
[full list](https://pandas.pydata.org/pandas-docs/stable/reference/io.html#flat-file) of input/output capabilities of **pandas**.

The main function for reading text files is `read_csv` (or `read_table`). 
Although the function name would indicate one can only read CSV files, it does read other standard text files as long as the delimiter is specified.
Quite a large number of optional arguments can be implemented for `read_csv`/`read_table`, some common ones are highlighted here for the sake of brevity. 
The full documentation can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).

- `filepath` (various): either a path to a file (`str`, `pathlib.Path`) URL, or any object with a `read()` method
- `sep` (str): delimiter to use, defaults to *','* for `read_csv` and *'\t'* for `read_table`
- `dtype` (data-type): data-type of the resulting array, default is None
- `skiprows` (int): number of lines at the beginning of the file to skip (includes comments), default is 0
- `nrows` (int): number of rows of file to read, default is to read all rows
- `header` (int): row numbers to use as column names, default is to infer column names from the first line of the file
- `names` (array-like): list of column names to use
- `index_col` (int, str, or False): column to use as row labels of the `DataFrame`, default is to infer
- `usecols` (list-like): returns a subset of columns

Let's import a data file with information about office supply ordering (found under *Workshop_Day1/Data/office_supplies.csv*)

```
office_supplies = pd.read_csv('./Data/office_supplies.csv')
office_supplies
```

Note, **pandas** correctly infers the first row of the CSV file is the header information, rather than actual data. 
There is an additional column of index values that has been prepended to our `DataFrame`, which corresponding to the row number.

We can indicate the data type for the entire `DataFrame`, or individual columns with the `dtype` parameter. To set the entire `DataFrame`, we would pass `dtype=datatype` (i.e.,
`dtype=object` makes everything an **object**). To specify individual columns, use `dtype={'Column':datatype}`, where `'Column'` is the column header. 
However, **pandas** is pretty good at inferring the data type on it's own. We can double check that the data types are what we want with `dtypes`.

```
office_supplies.dtypes
```

We can indicate specific column header names by using the `names` argument. 
Let's demonstrate with the CSV of fruits with and without peels we used in the previous section (found under *Workshop_Day1/Data/fruits.csv*).

```
fruits = pd.read_csv('./Data/fruits.csv', skiprows=1, names=['Peels', 'No Peels'])
fruits
```

Here, I've skipped the first row since it's a commented header (`skiprows=1`) and then defined a list of strings to use with `names` to set the column headers.

To read Microsoft Excel files (*.xlsx*), we can use the `read_excel` method from **pandas**. As an example, let's import some data on safety incidents over three months
(found under *Workshop_Day1/Data/safety_incidents.xlsx*). This method has us specify the name of the sheet we want to read, as well as the file name.

```
jan_incidents = pd.read_excel('./Data/safety_incidents.xlsx', sheet_name='January')
jan_incidents
```

Otherwise, the optional parameters for `read_excel` resemble those of `read_csv`.

A more efficient way of working with an Excel file with multiple sheets is to use the `ExcelFile` class so the file is stored in the memory only one time.

```
incidents = pd.ExcelFile('./Data/safety_incidents.xlsx')
feb_incidents = pd.read_excel(incidents, 'February')
feb_incidents
```

With `ExcelFile`, we can use `sheet_names` to get a list of the sheet names in the file.

```
incidents.sheet_names
```

See the documentation for more on [read_excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) and 
[ExcelFile](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelFile.html).

## 1.1.2 Querying data

### Querying files with numpy
---
To access individual elements in a numpy **array**, we can use array indexing. Like lists, arrays are **zero-indexed**, meaning we start indexing (counting) elements at 0 instead of 1.
As an example, if we have the array `arr = [7.3, 11.2, 9.0, 3.5]`, the index of the first element, 7.3, is 0 while the index of the last element, 3.5, is 3.
We denote the index in square brackets, so `arr\[0\] = 7.3` and `arr\[3\] = 3.5`. 

Let's return to *rand_gaussian.txt*, which we imported as *data*, to further demonstrate. Say I want to print each value in this dataset with it's corresponding index.
This can be acheived using a **for loop** that iterates over the length of the array: 

```
for i in range(len(data)):
	print(i, data[i])
```

In the above for loop, we use *i* as a placeholder for the current index for where we are in the length of the array. 
Then, in the print statement, we first print the value of the index (*i*) and then the value in the data array corresponding to that index (*data[i]*).

For a two-dimensional array, we simply use a second index two denote the second dimension. In other words, `arr[i,j]` corresponds to the value in *i*th row and *j*th column of the array.
For an example, let's return to the diffraction data imported from *Si_Expt.txt* as *data_2d*. 
To print the first pair of 2θ and intensity values, or the first row in the text file, we can use the following code:

```
print(data_2d[0,0], data_2d[0,1])
```

Where the first bracketed value corresponds to the row index and the second bracketed value corresponds to the column index. 
So, in this case, we're printing the value in row 0 column 0, and the value in row 0 column 1.

We can also grab the entirety of a given row in a 2-D array by only including the first index value (row), and not the second. The following prints the first row:

```
print(data_2d[0])
```

This is called a slice.

Similarly, we can get an entire column of a 2-D array as a slice by having the first index value as *':'*. The following prints the first column:

```
print(data_2d[:,0])
```

Indexing can also be used to assign values to specific elements in the array. For example, `data_2d[0,0] = 1.0` would overwrite the value originally at [0,0] with 1.0.
The same can be done with slices, `data_2d[:,0] = 1.0` would replace every value in the first column with 1.0.

The above principles extend to *n*-dimensional arrays.

Another useful tool is the `dtype` property, which returns the data type of an array.

```
print(data.dtype)
```

The data type of an array can be converted to another type using `astype`. The following returns the array *data* (float-type array) as a string-type array:

```
str_data = data.astype(str)
print(str_data)
```

For more basics on the **NumPy** package, see the [NumPy quickstart page](https://numpy.org/doc/stable/user/quickstart.html).

### Querying files with pandas
---
Indexing allows us to select lower-dimensional slices from a data structure. In the case of `DataFrames`, we can use indexing to retrieve a `Series`. 
Similar to indexing a **numpy array**, we can index a `Series` to get a scalar value. This is summarized below:

| Object Type | Selection        | Result                            |
| ----------- | ---------------- | --------------------------------- |
| `Series`    | `series[label]`  | scalar value                      |
| `DataFrame` | `frame[colname]` | `Series` corresponding to colname |

Let's use the office supplies CSV file from section 1.1.1 (found under *Workshop_Day1/Data/office_supplies.csv*) to demonstrate.

Getting and setting entire columns can be done using `frame[colname]`. The following code returns the column containing ordered items as a `Series`:

```
items = office_supplies['Item']
print(items)
```

Multiple columns can simultaneously be retrieved by passing a list of column names.

```
reps_totals = office_supplies[['Rep', 'Total']]
print(reps_totals)
```

Further, we can get single values from *items* by indexing, as we did with arrays. 
Since the row labels are index numbers (0, 1, 2, ...), we use the index number of the row we're interested in.

```
first_item = items[0]
print(first_item)
```

Similarly, if we set `office_supplies['Total']` equal to another `DataFrame` of equivalent size, we replace the values in the `'Total'` column of `office_supplies`.
If `office_supplies['Total']` is set to equal a scalar value, it will be propagated to fill the column. 
That is, if we set `office_supplies['Total']` equal to 123.45, all the original values in the `'Total'` column are replaced with 123.45.

Another choice for indexing is the `.loc` function, which is based on **labels**.

As an example, let's import the data file *employees.csv* (found under *Workshop_Day1/Data/employees.csv*) and set the row labels as each employee's name, instead of an index number.

```
employees = pd.read_csv('./Data/employees.csv', index_col=0)
employees
```

Now, we can use `.loc` with a name and get all the information about that particular employee.

```
employees.loc['Emily']
```

Alternatively, we can get row elements based on **integer position** using `.iloc`.

```
employees.iloc[0]
```

For additional information on indexing with a `DataFrame` object and more advanced methods, 
see the [documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#combining-positional-and-label-based-indexing).

## 1.1.3 Debugging common data importing issues

---TO ADD---

numbers read as strings
header shifts file lines/indexing

## 1.1.4 Importing multiple files simultaneously

---TO ADD---

---
### Congratulations!  
You have now completed 1.1 External Data files.